{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b1831cd",
   "metadata": {},
   "source": [
    "# Kursdatenalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f279b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 632 courses\n"
     ]
    }
   ],
   "source": [
    "datapath = \"crawler/database/semester_20251\"\n",
    "\n",
    "# get a list of all json files in the directory  \n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_course_objects(datapath):\n",
    "    json_files = []\n",
    "    for file in os.listdir(datapath):\n",
    "        if file.endswith(\".json\"):\n",
    "            course = {}\n",
    "            with open(os.path.join(datapath, file), 'r') as f:\n",
    "                course = json.load(f)\n",
    "                course[\"html\"] = file.replace(\".json\", \".html\")\n",
    "            json_files.append(course)\n",
    "    return json_files\n",
    "\n",
    "# get the course objects\n",
    "courses = get_course_objects(datapath)\n",
    "# filter for courses with SWS defined\n",
    "courses = [course for course in courses if \"Grunddaten zur Veranstaltung\" in course and course[\"Grunddaten zur Veranstaltung\"][\"SWS\"] != \"missing\" and \"Veranstaltungstitel\" in course]\n",
    "\n",
    "print(\"Loaded \" + str(len(courses)) + \" courses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d7ca183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bisonlink': 'https://bison-connector.bauhaus.uni-weimar.de/qisserver/rds?state=verpublish&status=init&vmfile=no&publishid=65070&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung',\n",
      " 'Fakultät': 'Fakultät Medien',\n",
      " 'Grunddaten zur Veranstaltung': {'Erwartete Teilnehmer/-innen': 'missing',\n",
      "                                  'Hyperlink': 'http://www.uni-weimar.de/vsp',\n",
      "                                  'Max. Teilnehmer/-innen': 'missing',\n",
      "                                  'Rhythmus': 'jedes 2. Semester',\n",
      "                                  'SWS': '2',\n",
      "                                  'Semester': 'SoSe 2025',\n",
      "                                  'Sprache': 'englisch',\n",
      "                                  'Veranstaltungsart': 'Übung',\n",
      "                                  'Veranstaltungsnummer': '2909035/02',\n",
      "                                  'Zugeordnetes Modul': 'missing'},\n",
      " 'Personen': [{'faculty': 'Fakultät Bau- und Umweltingenieurwissenschaften',\n",
      "               'regular_name': 'Leon Thiebes'},\n",
      "              {'faculty': 'Fakultät Bau- und Umweltingenieurwissenschaften',\n",
      "               'regular_name': 'Marco Fedior'},\n",
      "              {'faculty': 'Fakultät Bau- und Umweltingenieurwissenschaften',\n",
      "               'regular_name': 'Uwe Plank-Wiedenbeck'},\n",
      "              {'faculty': 'Fakultät Bau- und Umweltingenieurwissenschaften',\n",
      "               'regular_name': 'Julius Uhlmann'}],\n",
      " 'Veranstaltungstitel': 'Software-based Simulation of Traffic and Emissions',\n",
      " 'Weitere Angaben zur Veranstaltung': {'Bemerkung': 'Gemeinsam mit dem Teil '\n",
      "                                                    '\"Fundamentals of '\n",
      "                                                    'Microscopic Traffic '\n",
      "                                                    'Simulations\" umfasst das '\n",
      "                                                    'Modul \"Microscopic '\n",
      "                                                    'Traffic Simulation\" 4 SWS '\n",
      "                                                    'und 6 LP.',\n",
      "                                       'Beschreibung': 'Beleg: '\n",
      "                                                       'Softwarebasierte '\n",
      "                                                       'Simulation von Verkehr '\n",
      "                                                       'und '\n",
      "                                                       'Emissionen•\\xa0\\xa0\\xa0 '\n",
      "                                                       'Einen '\n",
      "                                                       'vorfahrtsgeregelten '\n",
      "                                                       'Knotenpunkt von Grund '\n",
      "                                                       'auf '\n",
      "                                                       'modellieren•\\xa0\\xa0\\xa0 '\n",
      "                                                       'Einen '\n",
      "                                                       'vorfahrtsgeregelten '\n",
      "                                                       'Knotenpunkt simulieren '\n",
      "                                                       'und '\n",
      "                                                       'bewerten•\\xa0\\xa0\\xa0 '\n",
      "                                                       'Einen signalisierten '\n",
      "                                                       'Knotenpunkt '\n",
      "                                                       'modellieren, '\n",
      "                                                       'simulieren und '\n",
      "                                                       'bewerten•\\xa0\\xa0\\xa0 '\n",
      "                                                       'Ein bestehendes Modell '\n",
      "                                                       'anpassen und '\n",
      "                                                       'simulieren•\\xa0\\xa0\\xa0 '\n",
      "                                                       'Ein bestehendes Modell '\n",
      "                                                       'kalibrieren und '\n",
      "                                                       'validieren•\\xa0\\xa0\\xa0 '\n",
      "                                                       'Eine '\n",
      "                                                       'Verkehrsmanagementmaßnahme '\n",
      "                                                       'bewerten',\n",
      "                                       'Leistungsnachweis': 'Im Modulteil '\n",
      "                                                            '„Software-based '\n",
      "                                                            'Simulation of '\n",
      "                                                            'Traffic and '\n",
      "                                                            'Emissions“ sind '\n",
      "                                                            'semesterbegleitende '\n",
      "                                                            'Belege '\n",
      "                                                            'anzufertigen. Die '\n",
      "                                                            'Vorlesung '\n",
      "                                                            '„Fundamentals of '\n",
      "                                                            'Microscopic '\n",
      "                                                            'Traffic '\n",
      "                                                            'Simulation“ '\n",
      "                                                            'schließt mit '\n",
      "                                                            'einer '\n",
      "                                                            'schriftlichen '\n",
      "                                                            'Prüfung (60\\u202f'\n",
      "                                                            'Minuten) ab. Die '\n",
      "                                                            'Belege sind '\n",
      "                                                            'Prüfungsvoraussetzung. '\n",
      "                                                            'Die Modulnote '\n",
      "                                                            'setzt sich aus '\n",
      "                                                            'der Note der '\n",
      "                                                            'Belege (50\\u202f'\n",
      "                                                            '%) und der '\n",
      "                                                            'Prüfung (50\\u202f'\n",
      "                                                            '%) zusammen. Eine '\n",
      "                                                            'Einzelbelegung '\n",
      "                                                            'der beiden '\n",
      "                                                            'Modulteile ist '\n",
      "                                                            'nicht möglich.',\n",
      "                                       'Literatur': 'Treiber, M. (2013): '\n",
      "                                                    'Traffic flow dynamics: '\n",
      "                                                    'data, models and '\n",
      "                                                    'simulationBeyer, J. '\n",
      "                                                    '(2015): Cybernetics in '\n",
      "                                                    'planning and operation to '\n",
      "                                                    'assist prospective public '\n",
      "                                                    'transportation systems, '\n",
      "                                                    'International Conference '\n",
      "                                                    'on Modeling the Future of '\n",
      "                                                    'Ho Chi Minh City, Binh '\n",
      "                                                    'Duong New City, Binh '\n",
      "                                                    'Duong Province, Vietnam, '\n",
      "                                                    'September 2015, ISBN: '\n",
      "                                                    '978-604-913-414-2PTV AG: '\n",
      "                                                    'PTV Vissim 2020 User '\n",
      "                                                    'ManualCURRENT RULES AND '\n",
      "                                                    'REGULATIONS OF THE GERMAN '\n",
      "                                                    'RESEARCH SOCIETY FOR ROAD '\n",
      "                                                    'AND TRAFFIC ENGINEERING '\n",
      "                                                    '(FGSV): Hinweise zur '\n",
      "                                                    'Datenvervollständigung '\n",
      "                                                    'und Datenaufbereitung in '\n",
      "                                                    'verkehrstechnischen '\n",
      "                                                    'Anwendungen (Nr. 382); '\n",
      "                                                    'Arbeitspapier – Data '\n",
      "                                                    'Mining im '\n",
      "                                                    'Verkehrsmanagement und in '\n",
      "                                                    'der Verkehrsplanung: '\n",
      "                                                    'Anwendungen und Verfahren '\n",
      "                                                    '(Nr. 382/2); Hinweise zur '\n",
      "                                                    'mikroskopi-schen '\n",
      "                                                    'Verkehrsflusssimulation – '\n",
      "                                                    'Grundlagen und Anwendung '\n",
      "                                                    '(Nr. 388), '\n",
      "                                                    'e.g.Umweltbundesamt:Handbook '\n",
      "                                                    'Emission Factors for Road '\n",
      "                                                    'Transport – HBEFA (2019)'},\n",
      " 'Zeit': {'Bemerkung': '',\n",
      "          'Tag': 'Mo.',\n",
      "          'Terminrhythmus': 'unger. Wo',\n",
      "          'Zeit': '13:30 bis 16:45'},\n",
      " 'html': '2909035-02.html'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(courses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8c0e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = [\n",
    "    course[\"Veranstaltungstitel\"] + \" \" + course[\"Weitere Angaben zur Veranstaltung\"][\"Beschreibung\"]\n",
    "    for course in courses\n",
    "    if \"Weitere Angaben zur Veranstaltung\" in course and \"Beschreibung\" in course[\"Weitere Angaben zur Veranstaltung\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b621bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/llorenz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n",
    "\n",
    "# Download German stopwords if not already done\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Combine English and German stopwords\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS) + list(stopwords.words('german'))\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "533c1441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /home/llorenz/.venvs/main/lib/python3.13/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/llorenz/.venvs/main/lib/python3.13/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/llorenz/.venvs/main/lib/python3.13/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/llorenz/.venvs/main/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e5f4056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llorenz/.venvs/main/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-24 15:48:37,657 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 18/18 [01:13<00:00,  4.06s/it]\n",
      "2025-06-24 15:49:57,789 - BERTopic - Embedding - Completed ✓\n",
      "2025-06-24 15:49:57,791 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-06-24 15:50:15,366 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-06-24 15:50:15,377 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-06-24 15:50:15,516 - BERTopic - Cluster - Completed ✓\n",
      "2025-06-24 15:50:15,545 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-06-24 15:50:15,984 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic  Count                                               Name  \\\n",
      "0       -1     44               -1_garage_neudeli_bauhaus_kolloquium   \n",
      "1        0     17                         0_vob_werfen_tool_elements   \n",
      "2        1     15                     1_moderne_texte_events_seminar   \n",
      "3        2     14                   2_höhlen_forest_grundlagen_reset   \n",
      "4        3     14             3_usability_projektmodul_spot_bachelor   \n",
      "..     ...    ...                                                ...   \n",
      "96      95      3      95_bestimmte_interface_praktiken_menschlichen   \n",
      "97      96      3              96_structural_sensor_steel_structures   \n",
      "98      97      3                       97_virtual_vr_reality_agents   \n",
      "99      98      3  98_metallkorrosion_korrosionsschutz_aktiver_ge...   \n",
      "100     99      2         99_sustainability_diversity_urban_language   \n",
      "\n",
      "                                        Representation  \\\n",
      "0    [garage, neudeli, bauhaus, kolloquium, archite...   \n",
      "1    [vob, werfen, tool, elements, queer, antwerp, ...   \n",
      "2    [moderne, texte, events, seminar, kultur, kuns...   \n",
      "3    [höhlen, forest, grundlagen, reset, excursion,...   \n",
      "4    [usability, projektmodul, spot, bachelor, obje...   \n",
      "..                                                 ...   \n",
      "96   [bestimmte, interface, praktiken, menschlichen...   \n",
      "97   [structural, sensor, steel, structures, proper...   \n",
      "98   [virtual, vr, reality, agents, recordings, use...   \n",
      "99   [metallkorrosion, korrosionsschutz, aktiver, g...   \n",
      "100  [sustainability, diversity, urban, language, c...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [Garagenkinder - Gründungsmythen im Digitalzei...  \n",
      "1    [Typtopia - Warsaw Kooperationsworkshop für Ty...  \n",
      "2    [Kultur- und Kunstsoziologie In den (Theorie-)...  \n",
      "3    [Felsmechanik - Felsbau - Tunnelbau Ingenieurg...  \n",
      "4    [Good Dog! – Teaching new Tricks to Spot the R...  \n",
      "..                                                 ...  \n",
      "96   [\"I prefer not to\" - Erschöpfung, Ohnmacht, Pa...  \n",
      "97   [Experimental structural dynamics and Structur...  \n",
      "98   [VRinSync Collaborative activities such as dan...  \n",
      "99   [Teil: Stabilität plattenartiger Stahlbauteile...  \n",
      "100  [Was ist Universität? Geschichte, Konflikt, Ar...  \n",
      "\n",
      "[101 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "umap_model = UMAP(n_neighbors=2, n_components=5, min_dist=0.4, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=2)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    language=\"multilingual\",\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True\n",
    ")\n",
    "topics, probs = topic_model.fit_transform(descriptions)\n",
    "\n",
    "# Show topic info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab6ff11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: '-1_garage_neudeli_bauhaus_kolloquium', 0: '0_vob_werfen_tool_elements', 1: '1_moderne_texte_events_seminar', 2: '2_höhlen_forest_grundlagen_reset', 3: '3_usability_projektmodul_spot_bachelor', 4: '4_chemnitz_europäischen_stadt_städtebaulichen', 5: '5_fulldome_filme_imagination_imaginieren', 6: '6_urban_cities_german_stadtsoziologie', 7: '7_animation_klang_non_sehens', 8: '8_software_model_argumentation_engineering', 9: '9_musik_musikpsychologie_hfm_csound', 10: '10_örr_regulierung_informations_medienökonomik', 11: '11_film_language_processing_bergbau', 12: '12_forschungskolloquium_studien_markenführung_forschungs', 13: '13_bounds_gamesfabrik_technologie_bauhaus', 14: '14_concrete_building_materials_damage', 15: '15_haus_pappeln_van_hohe', 16: '16_sprachumschaltflagge_englischsprachigen_ku_installation', 17: '17_class_00_join_pcbs', 18: '18_bauens_architektur_sustainability_schwelle', 19: '19_stochastic_reliability_simulation_power', 20: '20_städtebau_ost_thüringen_knotenpunktsystem', 21: '21_mediengeschichte_medienkunst_medien_medienwissenschaft', 22: '22_resistance_prekarität_power_carnivalesque', 23: '23_hybrid_stroh_umwelt_baustoffe', 24: '24_komplexitätstheorie_printing_3d_hochdruck', 25: '25_objects_code_home_methods', 26: '26_maps_mapping_product_used', 27: '27_musikwissenschaft_jahrhundert_half_jazz', 28: '28_persönlicher_verteidigung_anmeldung_ba', 29: '29_hamburg_manifestos_brandschutz_wohnungsbau', 30: '30_digitale_digitaler_gewalt_freiekunst_weimar', 31: '31_malerei_projekt_arbeitsplatz_zeichnung', 32: '32_denkmalpflege_denkmal_beispiel_besondere', 33: '33_coudray_hfm_konzert_chor', 34: '34_aubrac_dritten_dritte_lyon', 35: '35_landschaft_industrie_tempelhofer_feld', 36: '36_planning_gis_instrumente_processes', 37: '37_anlagen_abfallbehandlung_industrieabwasserreinigung_exforma', 38: '38_formate_berlin_gossip_festival', 39: '39_stadtplanung_professur_fragestellung_masterarbeit', 40: '40_management_unternehmen_iwm_geld', 41: '41_water_stoffinhalte_ww_hochwasserschutz', 42: '42_ai_story_ki_portfolioerstellung', 43: '43_energiesystemen_gebäudeplanung_energetischen_modellierung', 44: '44_freies_projekt_betreuenden_lehrenden', 45: '45_landschaftsarchitektur_handlungsfelder_urbaner_bewertung', 46: '46_projektmodule_vierten_fachlich_siebten', 47: '47_1960plus_denkmalpflegerischen_studierendenwettbewerb_icomos', 48: '48_dornburg_nachhaltigen_tektonik_lokal', 49: '49_ringvorlesung_materialisierung_wohnens_wandel', 50: '50_thesis_seminare_bachelor_anfrage', 51: '51_futures_design_speculative_fiction', 52: '52_sha_hashfunktionen_kandidaten_hashing', 53: '53_masterarbeit_fragestellung_themeneingrenzung_wissensbestandteilen', 54: '54_infrastrukturrecht_fragestellungen_eu_bereitstellung', 55: '55_kryptographie_komponenten_schwierig_kryptographischer', 56: '56_hören_wahrnehmung_sound_klang', 57: '57_storytelling_editorial_fotografie_composition', 58: '58_wenig_konsultationen_selber_darauffolgenden', 59: '59_timber_bearing_load_stahl', 60: '60_stadtraumkw_05_06_weimar', 61: '61_sesc_schwellen_flingern_schaffen', 62: '62_optimization_problems_diskrete_optimierungsprobleme', 63: '63_zeitschrift_beiträge_titles_inhouse', 64: '64_schrift_variable_interaktion_mr', 65: '65_presenter_open_handarbeit_responders', 66: '66_ziegel_hergestellt_vogtland_ziegelrohlinge', 67: '67_urbane_climate_kommunikationsstrategien_veränderung', 68: '68_sound_professionelle_listening_heilung', 69: '69_beton_stahlbetonelementen_durchbildung_konstruktive', 70: '70_vorprüfung_ph_künstlerisch_präzisiert', 71: '71_hazard_soil_flood_assessment', 72: '72_forschung_innen_ansätze_kopffüßer', 73: '73_counter_mappings_flinta_erhebungen', 74: '74_dokumentation_10_12_filmmaterial', 75: '75_potsdam_bau_unterkunft_reisekosten', 76: '76_flüchtlinge_flüchtlingen_unterrichts_unterricht', 77: '77_planning_decision_urban_amsterdam', 78: '78_wohnen_skizzen_illustrator_holme', 79: '79_städtebaus_städtischen_stadt_großartige', 80: '80_bilder_generative_programming_wirklichkeit', 81: '81_image_logos_website_animationen', 82: '82_video_workflows_belcim_macro', 83: '83_straßenentwurf_verkehrsmanagement_wasserversorgungs_lichtsignalanlagen', 84: '84_städte_prozent_profil_ziele', 85: '85_baukonstruktion_detailpunkte_übung_vorlesungsreihe', 86: '86_bachelorarbeit_fragestellung_themeneingrenzung_wissensbestandteilen', 87: '87_licht_beleuchtung_wieviel_grundgrößen', 88: '88_verbundbrücken_stahl_berechnung_behälter', 89: '89_scores_fluxus_1963_tun', 90: '90_gerechtigkeit_recht_medialisierungsprozesse_medien', 91: '91_performances_formate_poetryfilmtage_poetische', 92: '92_rekonstruktionen_denkmalpflege_reparatur_gegangen', 93: '93_straßenentwurf_vehicles_autonomous_einübung', 94: '94_medien_mediengeschichte_medienökologie_subjekts', 95: '95_bestimmte_interface_praktiken_menschlichen', 96: '96_structural_sensor_steel_structures', 97: '97_virtual_vr_reality_agents', 98: '98_metallkorrosion_korrosionsschutz_aktiver_gesetzsauerstoffkorrosion', 99: '99_sustainability_diversity_urban_language'}\n"
     ]
    }
   ],
   "source": [
    "topic_model.generate_topic_labels()\n",
    "labels = topic_model.topic_labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098fa32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Courses for topic 33 (33_coudray_hfm_konzert_chor):\n",
      "\n",
      "Title: 5. Kernmodul: Versuchsgut Dornburg – Experimente zu einer nachhaltigen Tektonik\n",
      "Link: https://bison-connector.bauhaus.uni-weimar.de/qisserver/rds?state=verpublish&status=init&vmfile=no&publishid=66767&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung\n",
      "\n",
      "Title: Der imaginierte Alltag - Genremalerei in den Niederlanden (auch Prüfungsmodul Lehramt)\n",
      "Link: https://bison-connector.bauhaus.uni-weimar.de/qisserver/rds?state=verpublish&status=init&vmfile=no&publishid=66030&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung\n",
      "\n",
      "Title: 5. Kernmodul: Maison du Peuple\n",
      "Link: https://bison-connector.bauhaus.uni-weimar.de/qisserver/rds?state=verpublish&status=init&vmfile=no&publishid=66016&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung\n",
      "\n",
      "Title: Digested Specimen - Turning the Music into design\n",
      "Link: https://bison-connector.bauhaus.uni-weimar.de/qisserver/rds?state=verpublish&status=init&vmfile=no&publishid=66079&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung\n",
      "\n",
      "Title: Dritte Orte - Eine offene Pforte für die lokale Kohorte. Entwicklung eines kulturellen Ortes in Aubrac.\n",
      "Link: https://bison-connector.bauhaus.uni-weimar.de/qisserver/rds?state=verpublish&status=init&vmfile=no&publishid=66157&moduleCall=webInfo&publishConfFile=webInfo&publishSubDir=veranstaltung\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show all links of courses assigned to label\n",
    "label = 33\n",
    "\n",
    "# Get indices of courses with this topic\n",
    "course_indices = [i for i, topic in enumerate(topics) if topic == label]\n",
    "\n",
    "# Print course titles and links for this topic\n",
    "print(f\"\\nCourses for topic {label} ({labels[label]}):\\n\")\n",
    "for idx in course_indices:\n",
    "    print(f\"Title: {courses[idx]['Veranstaltungstitel']}\")\n",
    "    print(f\"Link: {courses[idx]['Bisonlink']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e270de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:1737\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1733\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mConverting from Tiktoken\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1735\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1736\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:1631\u001b[39m, in \u001b[36mTikTokenConverter.converted\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Tokenizer:\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     tokenizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1632\u001b[39m     tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n\u001b[32m   1633\u001b[39m         [\n\u001b[32m   1634\u001b[39m             pre_tokenizers.Split(Regex(\u001b[38;5;28mself\u001b[39m.pattern), behavior=\u001b[33m\"\u001b[39m\u001b[33misolated\u001b[39m\u001b[33m\"\u001b[39m, invert=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1635\u001b[39m             pre_tokenizers.ByteLevel(add_prefix_space=\u001b[38;5;28mself\u001b[39m.add_prefix_space, use_regex=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   1636\u001b[39m         ]\n\u001b[32m   1637\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:1624\u001b[39m, in \u001b[36mTikTokenConverter.tokenizer\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     vocab_scores, merges = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1625\u001b[39m     tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:1600\u001b[39m, in \u001b[36mTikTokenConverter.extract_vocab_merges_from_model\u001b[39m\u001b[34m(self, tiktoken_url)\u001b[39m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1598\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m bpe_ranks = \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m byte_encoder = bytes_to_unicode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/tiktoken/load.py:148\u001b[39m, in \u001b[36mload_tiktoken_bpe\u001b[39m\u001b[34m(tiktoken_bpe_file, expected_hash)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     contents = \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     ret = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/tiktoken/load.py:48\u001b[39m, in \u001b[36mread_file_cached\u001b[39m\u001b[34m(blobpath, expected_hash)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m read_file(blobpath)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m cache_key = hashlib.sha1(\u001b[43mblobpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m()).hexdigest()\n\u001b[32m     50\u001b[39m cache_path = os.path.join(cache_dir, cache_key)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'encode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Use a multilingual model for German/English\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m classifier = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzero-shot-classification\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjoeddav/xlm-roberta-large-xnli\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Example candidate labels (expand as needed, can be hundreds!)\u001b[39;00m\n\u001b[32m      7\u001b[39m candidate_labels = [\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWriting\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFonts\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSustainability\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBiology\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMusic\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFilm\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mUrbanism\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mArchitecture\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mProgramming\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHistory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSociology\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPsychology\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mArt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDesign\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEngineering\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMathematics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPhysics\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mChemistry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPhilosophy\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mEducation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/pipelines/__init__.py:1049\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1046\u001b[39m             tokenizer_kwargs = model_kwargs.copy()\n\u001b[32m   1047\u001b[39m             tokenizer_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mtorch_dtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m         tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[32m   1054\u001b[39m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1032\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1029\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1032\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2025\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2022\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2023\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2278\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2276\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2277\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2278\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2280\u001b[39m     logger.info(\n\u001b[32m   2281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2282\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2283\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:108\u001b[39m, in \u001b[36mXLMRobertaTokenizerFast.__init__\u001b[39m\u001b[34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     94\u001b[39m     vocab_file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[32m    106\u001b[39m     mask_token = AddedToken(mask_token, lstrip=\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = vocab_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/tokenization_utils_fast.py:139\u001b[39m, in \u001b[36mPreTrainedTokenizerFast.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_file = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mvocab_file\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_special_tokens = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33madditional_special_tokens\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     fast_tokenizer = \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venvs/main/lib/python3.13/site-packages/transformers/convert_slow_tokenizer.py:1739\u001b[39m, in \u001b[36mconvert_slow_tokenizer\u001b[39m\u001b[34m(transformer_tokenizer, from_tiktoken)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[32m   1735\u001b[39m         vocab_file=transformer_tokenizer.vocab_file,\n\u001b[32m   1736\u001b[39m         additional_special_tokens=transformer_tokenizer.additional_special_tokens,\n\u001b[32m   1737\u001b[39m     ).converted()\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1740\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1741\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1742\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1743\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a multilingual model for German/English\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
    "\n",
    "# Example candidate labels (expand as needed, can be hundreds!)\n",
    "candidate_labels = [\n",
    "    \"Writing\", \"Fonts\", \"Sustainability\", \"Biology\", \"Music\", \"Film\", \"Urbanism\", \"Architecture\", \"Programming\", \"History\", \"Sociology\", \"Psychology\", \"Art\", \"Design\", \"Engineering\", \"Mathematics\", \"Physics\", \"Chemistry\", \"Philosophy\", \"Education\"\n",
    "]\n",
    "\n",
    "# For each course description, get the top N labels\n",
    "for i, desc in enumerate(descriptions[:5]):  # Try on first 5 for speed\n",
    "    result = classifier(desc, candidate_labels, multi_label=True)\n",
    "    print(f\"Course: {courses[i]['Veranstaltungstitel']}\")\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        if score > 0.3:  # threshold for relevance\n",
    "            print(f\"  {label}: {score:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4d74a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Installing collected packages: protobuf\n",
      "Successfully installed protobuf-6.31.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install tiktoken\n",
    "!pip install protobuf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
